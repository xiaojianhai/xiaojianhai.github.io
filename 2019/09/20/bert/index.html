<!DOCTYPE html><html lang="zh-cn"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="bert"><meta name="keywords" content="paper"><meta name="author" content="Xu conglei"><meta name="copyright" content="Xu conglei"><title>bert | Xuconglei</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#结构和双向性"><span class="toc-number">2.</span> <span class="toc-text">结构和双向性</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Task1-Masked-language-model"><span class="toc-number">2.1.</span> <span class="toc-text">Task1 Masked language model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Task2-Next-Sentence-Prediction"><span class="toc-number">2.2.</span> <span class="toc-text">Task2 Next Sentence Prediction</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://raw.githubusercontent.com/xiaojianhai/blog_images/master/bolg_index/new.jpg"></div><div class="author-info__name text-center">Xu conglei</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/xiaojianhai">Follow me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">11</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">9</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">9</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/xiaojianhai/blog_images/master/bolg_index/1.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Xuconglei</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">bert</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-09-20</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/paper/">paper</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>  这篇博客，我会深入的分析，BERT模型之所以带来效果提升的原因。</p>
<p>  在下面的介绍开始以前，我先把一些相关的论文以及结构放在下面，下面的博客内容将会引用下面的论文中的内容。</p>
<ol>
<li><p>“Transformer” 特征提取器，这是BERT的基础结构，也是BERT效果提升的根本原因，介绍Transformer的论文:<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a></p>
</li>
<li><p>”OpenAI GPT” 第一次用 “Transformer” 特征提取器在大规模的预训练数据上建立语言模型，然后应用到下游的任务，介绍GPT的论文：<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">OpenAI’s work</a>。</p>
</li>
<li><p>”BERT“ 在”OpenAI GPT” 基础上进行了几处改动，介绍BERT的论文：<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">Google‘s work</a>.</p>
</li>
</ol>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>什么是BERT?</p>
<p>  <strong>BERT</strong>（Bidirectional Encoder Representation from Transformer),它是由一些列<strong>Transformer</strong>堆叠在一起构成（只包含Transformer的Decoder),其中双向一词<br>是<strong>BERT</strong>与<strong>GPT</strong>的主要区别，因为<strong>BERT</strong>的<strong>self-attention</strong>在两个方向同时进行而不像<strong>GPT</strong>的主要区别，因为<strong>BERT</strong>的主要区别，因为<strong>BERT</strong>的<strong>self-attention</strong><br>在两个方向同时进行而不像<strong>GPT</strong>的主要区别，因为<strong>BERT</strong>的<strong>self-attention</strong>在两个方向同时进行而不像<strong>GTP**</strong>self-attention<strong>在两个方向同时进行而不像</strong>GTP**<br>具体的来说，对于input sentence:” i love to work nlp”，在<strong>GPT</strong>中 “love” 这个单词只与它的前一个单词”i“ 和它自己本身之间存在<strong>self-attention</strong>的关系，<br>而在<strong>BERT</strong>中单词”i“ 将与句子中的所有单词进行self-attention操作。下面将会进一步介绍GPT不能双向的原因。</p>
<h1 id="结构和双向性"><a href="#结构和双向性" class="headerlink" title="结构和双向性"></a>结构和双向性</h1><p>  <strong>OpenAI GPT</strong> 在建立语言模型的任务基础上来学习参数建立预训练模型，不断提高网络在已知句子的前半部分的前提下预测下一个单词，如果我们在这里利用双向结构<br>建立语言模型，因为self-attention 机制的原因，在预测下一个单词前，这个单词已经和当前单词进行了self-attention操作，已经具有下一个单词的信息<br>从而预测的准确率会很快提高到100%，我们还是拿上面的”i love to  work nlp” 为例子，假如当前单词是“love” 我们首先利用一个双向的self-attention操作。下面将会进一步介绍GPT不能双向的原因。<br>将”i“，”love”,”to”,”work”,”nlp”，这个5个单词的信息嵌入到已知句子中，我们用求得的已知句子去预测下一个单词，因为“to”已经嵌入到上文中，信息已经泄露<br>准确率会很快到100%。</p>
<p>BERT 双向性的实现。</p>
<p>  通过上面对<strong>OpenAI</strong>的分析，我们可以得出一个结论；对于Transformer特征提取器加上双向来建立语言模型不可行，BERT在这里用了一个非常巧妙的手法，为模型加入双向性，<br>它改变了建立语言模型的方式，采取对句子进行随机MASK的”masked language model” 任务和预测下一句子类型的 “next sentence prediction” 的任务两个任务  </p>
<h2 id="Task1-Masked-language-model"><a href="#Task1-Masked-language-model" class="headerlink" title="Task1 Masked language model"></a>Task1 Masked language model</h2><p>BERT 会对输入的所有单词中的15%进行MASK操作，但是MASK操作并不相同，以”My Dog is hariy” 为例子。</p>
<blockquote>
<p>80% 将会被[mask]代替</p>
</blockquote>
<p>example： “My Dog is [mask]”</p>
<blockquote>
<p>10% 将会被随机单词代替</p>
</blockquote>
<p>example： “My Dog is rand”</p>
<blockquote>
<p>10% 将会保持不变</p>
</blockquote>
<p>example:  “My Dog is hariy”</p>
<p>为什么不只用[mask]字符这一种方式替代单词？</p>
<p>  因为如果在fine-tuning过程中，如果被[mask] 的单词没有出现，模型将不会知道在[mask]处存在单词的内容，将认为此处不需要任何输出，将会对模型的效果带来影响。<br>同时我们保留一部分不变的同时，能够让模型学习到mask单词的真正表示。random [mask] 的作用，用来比较模型[mask]的表现能力，使它表现能力比random的表现能力<br>优越。</p>
<p>几点缺点：</p>
<ol>
<li>因为每次都只选取15%的单词进行预测，而语言模型对所有的单词都进行预测，这样会要得到语言模型的loss需要更多次的迭代</li>
</ol>
<h2 id="Task2-Next-Sentence-Prediction"><a href="#Task2-Next-Sentence-Prediction" class="headerlink" title="Task2 Next Sentence Prediction"></a>Task2 Next Sentence Prediction</h2><p>该任务主要内容是，将两个句子输入到模型，然后去预测第二个句子与第一个句子是否同属与一个文本。</p>
<p>为什么需要第二个句子预测任务？</p>
<p> 对于一些句子生成类任务来说，想自动问答，自然语言推理等任务，句子之间的关系对模型有很大的的作用。选取50%的实际的相邻句子，<br> 50% 的随机抽取句子进行训练。</p>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/paper/">paper</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/09/24/basic-statistics-knowledge/"><i class="fa fa-chevron-left">  </i><span>机器学习常犯的统计错误</span></a></div><div class="next-post pull-right"><a href="/2019/09/11/logest_palindromic_substring/"><span>Substring</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/xiaojianhai/blog_images/master/bolg_index/1.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By Xu conglei</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://xiaojianhai.github.io/">blog</a>!</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>